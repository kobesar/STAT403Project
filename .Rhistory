x[,30] <- x[, 1] * 15 + runif(200)
index <- sample(30, 5)
x_sub <- x[, index]
coeff <- runif(5, -5, 5)
y <- c()
for(i in 1:5) {
y <- coeff[i] * x_sub[, i]
}
index <- sample(2:30, 4)
index <- sample(2:30, 4)
x_sub <- x[, index]
index <- sample(2:30, 4)
index <- sample(2:30, 4)
x <- matrix(nrow = 200, ncol = 30)
for(i in 1:29) {
x[,i] <- rnorm(200, runif(1, -100, 100), 5)
}
x[,30] <- x[, 1] * 15 + runif(200)
index <- sample(2:30, 4)
x_sub <- x[, index]
coeff <- runif(5, -5, 4)
y <- x[,1]
for(i in 1:4) {
y <- y + coeff[i] * x_sub[, i]
}
splt <- sample(nrow(x), nrow(x)* 0.8)
x_train <- x[splt,]
x_test <- x[-splt,]
y_train <- y[splt]
y_test <- y[-splt]
summary(regsubsets(y_train ~ ., data = data.frame(x_train), method = "forward", nvmax = 10))
summary(regsubsets(y_train ~ ., data = data.frame(x_train), method = "forward", nvmax = 10))$obj
summary(regsubsets(y_train ~ ., data = data.frame(x_train), method = "forward", nvmax = 10))$which
summary(regsubsets(y_train ~ ., data = data.frame(x_train), method = "forward", nvmax = 10))$outmat
summary(regsubsets(y_train ~ ., data = data.frame(x_train), method = "forward", nvmax = 10))$outmat[,10]
summary(regsubsets(y_train ~ ., data = data.frame(x_train), method = "forward", nvmax = 10))$outmat[10,]
data.frame(x_train)
lm(train_y ~ X1, data.frame(x_train))
lm(y_train ~ X1, data.frame(x_train))
summary(regsubsets(y_train ~ ., data = data.frame(x_train), method = "forward", nvmax = 10))$outmat[10,]
lm1 <- lm(y_train ~ X1 + X4 + X11 + X13 + X14 + X18 + X20 + X28 + X29 + X30, data.frame(x_train))
lm1
pred <- predict(lm1, data.frame(x_test))
data.frame(x_train)
data.frame(x_train)$X13
knitr::opts_chunk$set(echo = TRUE)
library(leaps)
set.seed(435)
x <- matrix(nrow = 200, ncol = 30)
for(i in 1:29) {
x[,i] <- rnorm(200, runif(1, -100, 100), 5)
}
x[,30] <- x[, 1] * 15 + runif(200)
index <- sample(2:29, 4)
x_sub <- x[, index]
coeff <- runif(5, -5, 4)
y <- x[,1]
for(i in 1:4) {
y <- y + coeff[i] * x_sub[, i]
}
splt <- sample(nrow(x), nrow(x)* 0.8)
x_train <- x[splt,]
x_test <- x[-splt,]
y_train <- y[splt]
y_test <- y[-splt]
summary(regsubsets(y_train ~ ., data = data.frame(x_train), method = "forward", nvmax = 10))$outmat[10,]
lm1 <- lm(y_train ~ X1 + X2 + X4 + X5 + X8 + X10 + X14 + X16 + X23 + X27, data.frame(x_train))
index <- c(1, 2, 4, 5, 8, 10, 14, 16, 23, 27)
x_test <- x_test[,index]
pred <- predict(lm1, data.frame(x_test))
data.frame(x_test)
x_test <- x_test[,index]
x_test <- x[-splt,]
x_test <- x_test[,index]
x_test <- data.frame(x_test)
x_test <- x_test[,index]
x_test <- x[-splt,]
x_test <- data.frame(x_test)
x_test <- x_test[,index]
pred <- predict(lm1, data.frame(x_test))
test_mse <- mean((pred - y_test) ^ 2)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(kableExtra)
data("iris")
binary <- read.csv("binary.csv")
vals <- c()
for(i in 1:10000) {
}
glm1 <- glm(admit ~ gpa + gre, data = binary, family = "binomial")
p0 <- predict(glm1, type = "response")
john_df <- data.frame(gpa = 2.3, gre = 700)
sam_df <- data.frame(gpa = 3.9, gre = 670)
vals <- c()
for(i in 1:10000) {
index <- sample(nrow(binary), nrow(binary), replace = T)
samp <- binary[index,]
fit <- glm(admit ~ gpa + gre, data = samp, family = "binomial")
vals[i] <- predict(fit, john_df, type = "response") - predict(fit, sam_df, type = "response")
}
test_stat <-  (predict(glm1, john_df, type = "response") - predict(glm1, sam_df, type = "response")) / sd(vals)
2 * pnorm(abs(test_stat), lower.tail = F)
knitr::opts_chunk$set(echo = TRUE)
library(leaps)
set.seed(435)
x <- matrix(nrow = 200, ncol = 30)
for(i in 1:29) {
x[,i] <- rnorm(200, runif(1, -100, 100), runif(200, 0, 50))
}
x[,30] <- x[, 1] * 15 + runif(200)
index <- sample(2:29, 4)
x_sub <- x[, index]
coeff <- runif(5, -5, 4)
y <- x[,1]
for(i in 1:4) {
y <- y + coeff[i] * x_sub[, i]
}
splt <- sample(nrow(x), nrow(x)* 0.8)
x_train <- x[splt,]
x_test <- x[-splt,]
y_train <- y[splt]
y_test <- y[-splt]
summary(regsubsets(y_train ~ ., data = data.frame(x_train), method = "forward", nvmax = 10))$outmat[10,]
lm1 <- lm(y_train ~ X1 + X3 + X8 + X10 + X13 + X16 + X19 + X21 + X26 + X30, data.frame(x_train))
index <- c(1, 3, 8, 10, 13, 16, 19, 21, 26, 30)
x_test <- data.frame(x_test)
x_test <- x_test[,index]
pred <- predict(lm1, data.frame(x_test))
test_mse <- mean((pred - y_test) ^ 2)
View(x_train)
data.frame(x_test)$X9
data.frame(x_train)$X9
View(x_train)
data.frame(x_train)$X9
install.packages("glmnet")
knitr::opts_chunk$set(echo = TRUE)
library(leaps)
library(glmnet)
set.seed(435)
lambda <- 10^seq(10, -2, length = 100)
ridge <- glmnet(x_train, y_train, alpha = 0, lambda = lambda)
View(ridge)
ridge$lambda[4]
coeff(ridge)[, 4]
coef(ridge)[, 4]
cv.glmnet?
e
?cv.glmnet
lambda <- 10^seq(10, -2, length = 100)
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0, lambda = lambda)
View(cv_ridge)
plot(cv_ridge)
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0)
plot(cv_ridge)
cv.out$lambda.min
cv_ridge$lambda.min
log(11.4718)
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0, lambda = lambda)
cv_ridge$lambda.min
lambda <- 10^seq(10, -2, length = 100)
ridge <- glmnet(x_train, y_train, alpha = 0)
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0)
plot(x = lambda, y = coefficients[1,])
coefficients <- matrix(NA, nrow = 30, ncol = 100)
for (i in 1:30) {
for (j in 1:100) {
coefficients[i, j] <- coef(ridge)[, j][i+1]
}
}
plot(x = lambda, y = coefficients[1,])
coeff <- matrix(NA, nrow = 30, ncol = 100)
for (i in 1:30) {
for (j in 1:100) {
coeff[i, j] <- coef(ridge)[, j][i+1]
}
}
plot(x = lambda, y = coeff[1,], type = "l", ylim = c(min(coeff), max(coeff)))
for(i in 2:30) {
lines(x = lambda, y = coeff[2,], col = i)
}
plot(x = lambda, y = coeff[1,], type = "l", ylim = c(min(coeff), max(coeff)))
for(i in 2:30) {
points(x = lambda, y = coeff[2,], col = i)
}
plot(x = lambda, y = coeff[1,], type = "l", ylim = c(min(coeff), max(coeff)))
for(i in 2:30) {
lines(x = lambda, y = coeff[2,], col = i)
}
plot(x = log(lambda), y = coeff[1,], type = "l", ylim = c(min(coeff), max(coeff)))
for(i in 2:30) {
lines(x = log(lambda), y = coeff[2,], col = i)
}
plot(x = log(lambda), y = coeff[1,], type = "l", ylim = c(min(coeff), max(coeff)))
for(i in 2:30) {
lines(x = log(lambda), y = coeff[i,], col = i)
}
plot(x = log(lambda), y = coeff[1,], type = "l", ylim = c(min(coeff), max(coeff)))
for(i in 2:30) {
lines(x = log(lambda), y = coeff[i,], col = i)
}
abline(y = 0)
plot(x = log(lambda), y = coeff[1,], type = "l", ylim = c(min(coeff), max(coeff)))
for(i in 2:30) {
lines(x = log(lambda), y = coeff[i,], col = i)
}
abline(h = 0)
plot(x = log(lambda), y = coeff[1,], type = "l", ylab = "Coefficients"
ylim = c(min(coeff), max(coeff)))
plot(x = log(lambda), y = coeff[1,], type = "l", ylab = "Coefficients",
ylim = c(min(coeff), max(coeff)))
for(i in 2:30) {
lines(x = log(lambda), y = coeff[i,], col = i)
}
abline(h = 0, lwd = 3, lty = 2)
plot(x = log(lambda), y = coeff[1,], type = "l", ylab = "Coefficients",
ylim = c(min(coeff), max(coeff)))
for(i in 2:30) {
lines(x = log(lambda), y = coeff[i,], col = i)
}
abline(h = 0, lwd = 2, lty = 2)
plot(x = log(lambda), y = coeff[1,], type = "l", ylab = "Coefficients",
ylim = c(min(coeff), max(coeff)))
for(i in 2:30) {
lines(x = log(lambda), y = coeff[i,], col = i)
}
abline(h = 0, lwd = 1, lty = 2)
plot(x = log(lambda), y = coeff[1,], type = "l", ylab = "Coefficients",
ylim = c(min(coeff), max(coeff)))
for(i in 2:30) {
lines(x = log(lambda), y = coeff[i,], col = i)
}
abline(h = 0, lwd = 1.5, lty = 2)
plot(x = log(lambda), y = coeff[1,], type = "l", ylab = "Coefficients",
ylim = c(min(coeff), max(coeff)), col = 1)
for(i in 2:30) {
lines(x = log(lambda), y = coeff[i,], col = i)
}
abline(h = 0, lwd = 1.5, lty = 2)
ridge <- glmnet(x_train, y_train, alpha = 0, lambda = lambda)
coeff <- matrix(NA, nrow = 30, ncol = 100)
for (i in 1:30) {
for (j in 1:100) {
coeff[i, j] <- coef(ridge)[, j][i+1]
}
}
plot(x = log(lambda), y = coeff[1,], type = "l", ylab = "Coefficients",
ylim = c(min(coeff), max(coeff)), col = 1)
plot(x = log(lambda), y = coeff[1,], type = "l", ylab = "Coefficients",
ylim = c(min(coeff), max(coeff)), col = 1)
for(i in 2:30) {
lines(x = log(lambda), y = coeff[i,], col = i)
}
abline(h = 0, lwd = 1.5, lty = 2)
lasso <- glmnet(x_train, y_train, alpha = 1, lambda = lambda)
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1)
coeff <- matrix(NA, nrow = 30, ncol = 100)
for (i in 1:30) {
for (j in 1:100) {
coeff[i, j] <- coef(lasso)[, j][i+1]
}
}
plot(x = log(lambda), y = coeff[1,], type = "l", ylab = "Coefficients",
ylim = c(min(coeff), max(coeff)), col = 1)
for(i in 2:30) {
lines(x = log(lambda), y = coeff[i,], col = i)
}
abline(h = 0, lwd = 1.5, lty = 2)
View(lasso)
View(coeff)
paste(c(1, 3, 6))
paste(c(1, 3, 6), sep = ", ")
pred <- predict(ridge, s = cv_ridge$lambda.min, newx = x_test)
View(x_test)
x_test <- data.frame(x[-splt])
pred <- predict(ridge, s = cv_ridge$lambda.min, newx = x_test)
View(x_test)
x_test <- data.frame(x[-splt, ])
pred <- predict(ridge, s = cv_ridge$lambda.min, newx = x_test)
x_test <- x[-splt, ]
pred <- predict(ridge, s = cv_ridge$lambda.min, newx = x_test)
test_mse <- mean((pred - y_train)^2)
View(x_train)
predict(ridge,s = cv_ridge$lambda.min)
predict(ridge,s = cv_ridge$lambda.min, newx = x_train)
predict(ridge,s = cv_ridge$lambda.min, newx = x_test)
x_test <- x[-splt, ]
pred <- predict(ridge, s = cv_ridge$lambda.min, newx = x_test)
test_mse <- mean((pred - y_test)^2)
pred <- predict(lasso, s = cv_lasso$lambda.min, newx = x_test)
test_mse <- mean((pred - y_test)^2)
View(coeff)
knitr::opts_chunk$set(echo = TRUE)
library(leaps)
library(glmnet)
set.seed(435)
x <- matrix(nrow = 200, ncol = 30)
for(i in 1:29) {
x[,i] <- rnorm(200, runif(1, -5, 5), 10)
}
x[,30] <- x[, 1] * 1.5 + rnorm(200)
index <- sample(2:29, 4)
x_sub <- x[, index]
coeff <- runif(5, -5, 4)
y <- x[,1]
for(i in 1:4) {
y <- y + coeff[i] * x_sub[, i]
}
y <- y + rnorm(200)
splt <- sample(nrow(x), nrow(x)* 0.8)
x_train <- x[splt,]
x_test <- x[-splt,]
y_train <- y[splt]
y_test <- y[-splt]
summary(regsubsets(y_train ~ ., data = data.frame(x_train), method = "forward", nvmax = 10))$outmat[10,]
lm1 <- lm(y_train ~ X1 + X3 + X5 + X6 + X9 + X13 + X18 + X22 + X26 + X28, data.frame(x_train))
ind <- c(1, 3, 5, 6, 9, 13, 18, 22, 26, 28)
x_test <- data.frame(x_test)
x_test <- x_test[,ind]
pred <- predict(lm1, data.frame(x_test))
test_mse <- mean((pred - y_test) ^ 2)
lm1 <- lm(y_train ~ X1 + X3 + X4 + X5 + X6 + X8 + X15 + X20 + X26 + X28, data.frame(x_train))
ind <- c(1, 3, 4, 5, 6, 8, 15, 20, 26, 28)
x_test <- data.frame(x_test)
x_test <- x_test[,ind]
x_test <- x[-splt,]
x_test <- data.frame(x_test)
x_test <- x_test[,ind]
pred <- predict(lm1, data.frame(x_test))
test_mse <- mean((pred - y_test) ^ 2)
lambda <- 10^seq(10, -2, length = 100)
ridge <- glmnet(x_train, y_train, alpha = 0, lambda = lambda)
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0)
coeff <- matrix(NA, nrow = 30, ncol = 100)
for (i in 1:30) {
for (j in 1:100) {
coeff[i, j] <- coef(ridge)[, j][i+1]
}
}
plot(x = log(lambda), y = coeff[1,], type = "l", ylab = "Coefficients",
ylim = c(min(coeff), max(coeff)), col = 1,
main = "Ridge Coefficients across Values of Lambda")
for(i in 2:30) {
lines(x = log(lambda), y = coeff[i,], col = i)
}
abline(h = 0, lwd = 1.5, lty = 2)
x_test <- x[-splt, ]
pred <- predict(ridge, s = cv_ridge$lambda.min, newx = x_test)
test_mse <- mean((pred - y_test)^2)
lasso <- glmnet(x_train, y_train, alpha = 1, lambda = lambda)
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1)
coeff <- matrix(NA, nrow = 30, ncol = 100)
for (i in 1:30) {
for (j in 1:100) {
coeff[i, j] <- coef(lasso)[, j][i+1]
}
}
plot(x = log(lambda), y = coeff[1,], type = "l", ylab = "Coefficients",
ylim = c(min(coeff), max(coeff)), col = 1,
main = "LASSO Coefficients across Values of Lambda")
for(i in 2:30) {
lines(x = log(lambda), y = coeff[i,], col = i)
}
abline(h = 0, lwd = 1.5, lty = 2)
View(coeff)
pred <- predict(lasso, s = cv_lasso$lambda.min, newx = x_test)
test_mse <- mean((pred - y_test)^2)
1000^2
setwd("C:/Users/kaije/OneDrive - UW/UW Classes/STAT 403/STAT403Project")
df <- read.csv("Data/31119913_National2020.csv")
View(df)
data <- read.csv("Data/31119913_National2020.csv")
cols <- str_split("age10
sex
educ18
income20
racism20
life
party
stanum
sizeplac
pres
weight
vote2016
votemeth
abortion
climatec", "\n")[[1]]
library(tidyverse)
cols <- str_split("age10
sex
educ18
income20
racism20
life
party
stanum
sizeplac
pres
weight
vote2016
votemeth
abortion
climatec", "\n")[[1]]
data_subset <- data %>%
select(cols)
View(data)
sum(data$pres == "Joe Biden")
sum(data$pres == "Joe Biden")/ 15351
# Boot of percent Biden
n <- nrow(data)
biden_perc <- c()
for(i in 1:10000) {
j <- sample(n, n, replace = T)
samp <- data_subset[j,]
num_biden <- sum(samp$pres == "Joe Biden")
biden_perc[i] <- num_biden / n
}
hist(biden_perc)
abline(h = 0.5)
for(i in 1:10000) {
j <- sample(n, n, replace = T)
samp <- data_subset[j,]
num_biden <- sum(samp$pres == "Joe Biden")
biden_perc[i] <- num_biden / n
}
hist(biden_perc)
abline(h = 0.5)
hist(biden_perc)
abline(v = 0.5)
hist(biden_perc)
set.seed(403)
for(i in 1:1000) {
j <- sample(n, n, replace = T)
samp <- data_subset[j,]
num_biden <- sum(samp$pres == "Joe Biden")
biden_perc[i] <- num_biden / n
}
hist(biden_perc)
mean(biden_perc)
median(biden_perc)
(samp$pres == "Joe Biden")
(samp$pres == "Joe Biden") * 1000
(samp$pres == "Joe Biden") * samp$weight
# Boot with weight
biden_perc2 <- c()
for(i in 1:1000) {
j <- sample(n, n, replace = T)
samp <- data_subset[j,]
num_biden <- sum((samp$pres == "Joe Biden") * samp$weight)
biden_perc2[i] <- num_biden / sum(samp$weight)
}
hist(biden_perc2)
mean(biden_perc2 > 0.5)
mean(biden_perc > 0.5)
sd(biden_perc2)
(mean(biden_per2) - 0.5) / sd(biden_perc2)
(mean(biden_perc2) - 0.5) / sd(biden_perc2)
mean(biden_perc2)
View(data_subset)
sum(is.na(data$pres))
sum(data$pres == "")
data$pres[478]
sum(data$pres == " ")
data_pres_subset <- data_subset %>%
filter(pres != " ")
# Initial boot with weight
biden_perc2 <- c()
for(i in 1:1000) {
j <- sample(n, n, replace = T)
samp <- data_pres_subset[j,]
num_biden <- sum((samp$pres == "Joe Biden") * samp$weight)
biden_perc2[i] <- num_biden / sum(samp$weight)
}
hist(biden_perc2)
mean(biden_perc2 > 0.5)
# Initial boot of percent Biden
n <- nrow(data_pres_subset)
# Initial boot with weight
biden_perc2 <- c()
for(i in 1:1000) {
j <- sample(n, n, replace = T)
samp <- data_pres_subset[j,]
num_biden <- sum((samp$pres == "Joe Biden") * samp$weight)
biden_perc2[i] <- num_biden / sum(samp$weight)
}
for(i in 1:1000) {
j <- sample(n, n, replace = T)
samp <- data_pres_subset[j,]
num_biden <- sum((samp$pres == "Joe Biden") * samp$weight)
biden_perc2[i] <- num_biden / sum(samp$weight)
}
hist(biden_perc2)
mean(biden_perc2 > 0.5)
mean(biden_perc2)
(mean(biden_perc2) - 0.5) / sd(biden_perc2)
for(i in 1:1000) {
j <- sample(n, n, replace = T)
samp <- data_pres_subset[j,]
num_biden <- sum(samp$pres == "Joe Biden")
biden_perc[i] <- num_biden / n
}
hist(biden_perc2)
abline(v = 0.5)
hist(biden_perc2)
abline(v = 0.5, col = "red")
hist(biden_perc2)
abline(v = 0.5, col = "red", lwd = 2, lty = 2)
ist(biden_perc2)
abline(v = 0.5, col = "red", lwd = 2)
hist(biden_perc2)
abline(v = 0.5, col = "red", lwd = 2)
sum(data$stanum == "California")
sum(data$stanum == "New York")
